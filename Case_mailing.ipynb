{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Intro\n",
    "CRISP-DM is a widely used model in data mining and analytics. In this project, the idea of this methodology will be adopted to play with the real-life dataset. According to the widely used CRISP-DM methodology, Business Understanding, Data Understanding and Data Preparation will be introduced in the first two sections, and then Modeling and Evaluation will be discussed.\n",
    "# 1. Business Understanding\n",
    "## 1.1 Problem and Solution\n",
    "1. Problem: find the siginificant features to incur prospect engaged (reply, click, open)\n",
    "2. Solution format: features leading to high measures (responses, scores etc.)\n",
    "\n",
    "## 1.2 Idea\n",
    "1. with respect to three different types of response features, we can build model respectively and analyze separately (like control experiment)\n",
    "2. in terms of evaluation, we can consider different types of measurements\n",
    "3. visulization tools can help for predictive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation and Understanding\n",
    "## 2.1 Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support both py2 and py3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "##import os\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ignore useless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dataset\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try\n",
    "data_try = pd.read_csv('sequence-mailings.csv', error_bad_lines=False)\n",
    "\n",
    "with open('./sequence-mailings.csv', 'r') as file:\n",
    "    rows = len(file.readlines())-1\n",
    "print(len(data_try)-rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "import csv\n",
    "\n",
    "with open('sequence-mailings.csv',\"r\") as csv_in, open('sequence-mailings-clean.csv', \"w\") as csv_out:\n",
    "    reader = csv.reader(csv_in)\n",
    "    writer = csv.writer(csv_out)\n",
    "    rows = (tuple(item for idx, item in enumerate(row) if idx not in [44, 45, 46]) for row in reader)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe that the data contain error or bad lines/columns: with `error_bad_lines` argument, it will remove 65 rows of data.\n",
    "- By checking, there are some noisy data in the last three columns followed by last feature\n",
    "- Clean the original dataset by dropping columns followed by `tag`, save as 'sequence-mailings-clean.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('sequence-mailings-clean.csv')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Features\n",
    "### Generalization\n",
    "There are 1023321 data entries with 44 features. So it's necessary to make a generalization on the features. In this project, the features can be grouped as bellow:\n",
    "1. **Time Features**: `replied_at`,`opened_at`, `clicked_at`, `thread_replied_at`, `delivered_at` -- timestamp \n",
    "2. **ID Features**: `id`, `message_id`, `parent_message_id` -- string\n",
    "3. **Email Features**: `to_domain`, `from`, `mailbox_id` -- string\n",
    "4. **Content Features**: `subject_customized`, `body_customized`, `includes_link` -- boolean; `subject_length`, `body_length`, `template_id` -- int or string\n",
    "5. **System Features**: `track_links`, `track_opens`, `is_thread_reply` -- boolean; `sequence_id`, `sequence_step_id`, `sequence_template_id`, `sequence_state_id` -- string; `sequence_order` -- int\n",
    "6. **Prospect Features**: `prospect_id`, `prospect_first_name`, `prospect_time_zone`, `prospect_gender`, `prospect_occupation`, `prospect_city`, `prospect_zip`, `prospect_country`, `prospect_website`, `persona`, `company_name`, , `industry`, `website`, `company_locality`, `company_tier` -- string,  ; `prospect_dob`, `prospect_opted_out` -- timestamp; `company_size` -- int; `tag` -- list\n",
    "\n",
    "### Filtering\n",
    "Intuitively, some of the above features can be dropped. For example, **ID Features**, **Email Feature**, and many of \"sequnce\"-related features can be dropped because they are just detailed information for the mails and won't tell us anything about our objective. Similarly, many of the **Prospect Features** can also be dropped since they are detailed information for the prospects.\n",
    "\n",
    "Reason to do so lies in the fact that detailed information or system information are not directly interacted with prospect and they can be controled from Outreach end. In addition, some of the categorical features contain a large number of categories with quite uniform distribution, while some contain less categories like `gender` but the valid data are not enough. In this project we can ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make first selection\n",
    "select = ['replied_at', 'opened_at', 'clicked_at', 'delivered_at', \n",
    "          'is_thread_reply', 'includes_link', 'sequence_order', \n",
    "          'subject_customized', 'body_customized', 'subject_length', 'body_length',  \n",
    "          'persona', 'company_tier']\n",
    "\n",
    "data_1 = data[select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a smaller dataset, next we need to transform some features to appropriate format and deal with missing data\n",
    "- convert to data type of datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['replied_at', 'opened_at', 'clicked_at', 'delivered_at']\n",
    "\n",
    "for t in time:\n",
    "    data_1[t] = pd.to_datetime(data_1[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deal with missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null = ['includes_link', 'subject_length', 'body_length']\n",
    "\n",
    "for n in null:\n",
    "    data_1[n] = data_1[n].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "For the **Time Feature**, we will consider `replied_at`, `opened_at` and `clicked_at` as response variables. It's natrual to convert the *datetime* data to binary data, 1 for valid engagement and 0 otherwise, since what really counts is \"whether the prospect engaged\", not \"when exactly the prospect engaged\".\n",
    "\n",
    "As for *datetime* type of `delivered_at`, here we make a reasonable assumption that \"Month\" and \"Hours\" are much more important, since \"Year\" cannot recur and \"Minutes\" or \"Seconds\" are too meticulous to control.\n",
    "\n",
    "For the other categorical features like `persona` and `company_tier`, we need to convert those data to appropriate format that we can analyze in the model. Here we will use *One Hot Encode*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transform `replied_at`/`opened_at`/`clicked_at` to `reply`/`open`/`click`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['reply'] = data_1['replied_at'].isnull().apply(lambda x: 0 if x == True else 1)\n",
    "data_1['open'] = data_1['opened_at'].isnull().apply(lambda x: 0 if x == True else 1)\n",
    "data_1['click'] = data_1['clicked_at'].isnull().apply(lambda x: 0 if x == True else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transform `delivered_at` to `deliver_month` and `deliver_hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['deliver_month'] = data_1['delivered_at'].dt.month\n",
    "data_1['deliver_hour'] = data_1['delivered_at'].dt.hour\n",
    "\n",
    "data_1['company_tier'].fillna('Undefined', inplace=True)\n",
    "data_1['company_tier'] = data_1['company_tier'].apply(lambda x: 'Undefined' if x in ['201-1000', 'SMB'] else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- correlation matrix\n",
    "\n",
    "According to the correlation matrix, we can have a general picture about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### correlation matrix\n",
    "cor = ['reply', 'open', 'click', 'deliver_month', 'deliver_hour', 'is_thread_reply', 'includes_link', \n",
    "       'sequence_order', 'subject_customized', 'body_customized', 'subject_length', 'body_length']\n",
    "corr = data_1[cor].corr()\n",
    "\n",
    "### set the size of the figure\n",
    "plt.figure(figsize=(16,8))\n",
    "### set the title\n",
    "plt.title(\"Correlation Matrix\")\n",
    "### set the font size\n",
    "sns.set(font_scale=1.5)\n",
    "### plot\n",
    "f7 = sns.heatmap(data=corr, cbar=True, annot=True, fmt='.2f', annot_kws={'size':15}, \n",
    "                 yticklabels=corr.columns, xticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Observation\n",
    "- very imbalanced data\n",
    "\n",
    "As is shown below, all three targets have very imbalanced distribution in the dataset. Note that for `click`, data points with value of 1 are too sparse. Hence, this project mainly focused on `reply` and `open`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_pickle('data_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(13,5), sharex=True)\n",
    "plt.subplots_adjust(wspace=1)\n",
    "\n",
    "sns.countplot(x='reply', data=data_1, ax=axes[0])\n",
    "sns.countplot(x='open', data=data_1, ax=axes[1])\n",
    "sns.countplot(x='click', data=data_1, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plots for different features vs target\n",
    "\n",
    "Here list a series of plots for different features vs target. As shown in the figures, it shows that the selected features can be effective indicators to some extent, complying with the previous intuitive assumptions. Also, `open` seems enjoy much better effectiveness from those features. It can make sense because in the dataset, `open` has a more balanced distribution than `reply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deliver_hour\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5), sharex=True)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "pd.crosstab(data_1.deliver_hour, data_1.reply).plot(kind='bar', ax=axes[0])\n",
    "pd.crosstab(data_1.deliver_hour, data_1.open).plot(kind='bar', ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deliver_month\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5), sharex=True)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "pd.crosstab(data_1.deliver_month, data_1.reply).plot(kind='bar', ax=axes[0])\n",
    "pd.crosstab(data_1.deliver_month, data_1.open).plot(kind='bar', ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sequence_order\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5), sharex=True)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "table = pd.crosstab(data_1.sequence_order, data_1.reply)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, ax=axes[0])\n",
    "\n",
    "table = pd.crosstab(data_1.sequence_order, data_1.open)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, ax=axes[1])\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.sequence_order.hist()\n",
    "plt.xlabel('Sequence_order')\n",
    "plt.ylabel('Frequence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subject_customized\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5), sharex=True)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "pd.crosstab(data_1.subject_customized, data_1.reply).plot(kind='bar', stacked=True, ax=axes[0])\n",
    "pd.crosstab(data_1.subject_customized, data_1.open).plot(kind='bar', stacked=True, ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## body_customized\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5), sharex=True)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "pd.crosstab(data_1.body_customized, data_1.reply).plot(kind='bar', stacked=True, ax=axes[0])\n",
    "pd.crosstab(data_1.body_customized, data_1.open).plot(kind='bar', stacked=True, ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## persona\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5), sharex=True)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "pd.crosstab(data_1.persona, data_1.reply).plot(kind='bar', ax=axes[0])\n",
    "pd.crosstab(data_1.persona, data_1.open).plot(kind='bar', ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## company_tier\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5), sharex=True)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "pd.crosstab(data_1.company_tier, data_1.reply).plot(kind='bar', ax=axes[0])\n",
    "pd.crosstab(data_1.company_tier, data_1.open).plot(kind='bar', ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the project will build models and evaluations for `reply` and `open` respectively. The workflow will be organized as follows:\n",
    "1. Encoding: *One Hot Encoding* \n",
    "2. Handling imbalanced data: under-sampling and over-sampling\n",
    "3. Feature Selection: RFE\n",
    "4. Model: Logistic Regression and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 For `replied_at`\n",
    "### Encoding and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "data_2 = pd.get_dummies(data_1, columns=['deliver_month', 'deliver_hour', 'persona', 'company_tier'],\n",
    "                        prefix=['deliver_month', 'deliver_hour', 'persona', 'tier'])\n",
    "data_2.drop(['replied_at', 'opened_at', 'clicked_at', 'delivered_at'], axis=1, inplace=True)\n",
    "\n",
    "drop_reply = ['is_thread_reply', 'open', 'click']\n",
    "keep_reply = [i for i in data_2.columns.values.tolist() if i not in drop_reply]\n",
    "\n",
    "data_reply = data_2[keep_reply]\n",
    "data_reply.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_reply = data_reply.loc[:, data_reply.columns != 'reply']\n",
    "y_reply = data_reply.loc[:, data_reply.columns == 'reply']\n",
    "\n",
    "X_reply_train, X_reply_test, y_reply_train, y_reply_test = train_test_split(X_reply, y_reply,\n",
    "                                                                            test_size=0.3, random_state=2019)\n",
    "columns_reply = X_reply_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Under-sampling Majority Class\n",
    "\n",
    "Since the number of \"1\" is much smaller than that of \"0\", it's a fast and easy way to balance the data by randomly selecting a subset of class \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "us = RandomUnderSampler(random_state=2019)\n",
    "\n",
    "us_X_reply, us_y_reply = us.fit_resample(X_reply_train, y_reply_train)\n",
    "us_X_reply_df = pd.DataFrame(data=us_X_reply, columns=columns_reply )\n",
    "us_y_reply_df = pd.DataFrame(data=us_y_reply, columns=['reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the number of resampled data\n",
    "print(\"Total number of undersampled data:\",len(us_X_reply_df))\n",
    "print(\"Number of no reply:\",len(us_y_reply_df[us_y_reply_df['reply']==0]))\n",
    "print(\"Number of reply:\",len(us_y_reply_df[us_y_reply_df['reply']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Over-sampling Minority Class\n",
    "\n",
    "By searching some researches or blogs, I have learned there is a popular algorithm in this direction: SMOTE (Synthetic Minority Oversampling Technique). Apart from copying the minor class, it works by creating synthetic samples via randomly choosing one of the k-nearest-neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "os = SMOTE(random_state=2019)\n",
    "\n",
    "os_X_reply, os_y_reply = os.fit_sample(X_reply_train, y_reply_train)\n",
    "os_X_reply_df = pd.DataFrame(data=os_X_reply, columns=columns_reply )\n",
    "os_y_reply_df = pd.DataFrame(data=os_y_reply, columns=['reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the number of resampled data\n",
    "print(\"Total number of oversampled data:\",len(os_X_reply_df))\n",
    "print(\"Number of no reply:\",len(os_y_reply_df[os_y_reply_df['reply']==0]))\n",
    "print(\"Number of reply:\",len(os_y_reply_df[os_y_reply_df['reply']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "- RFE(Recursive Feature Elimination)\n",
    "\n",
    "The goal of RFE is to select features by recursively training models and pruning the least important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## under-sampling\n",
    "lg_us = LogisticRegression()\n",
    "\n",
    "rfe_us = RFE(lg_us, 20)\n",
    "rfe_us = rfe_us.fit(us_X_reply, us_y_reply.ravel())\n",
    "\n",
    "feature_reply_us = columns_reply[np.ix_(rfe_us.support_)]\n",
    "\n",
    "## over-sampling\n",
    "lg_os = LogisticRegression()\n",
    "\n",
    "rfe_os = RFE(lg_os, 20)\n",
    "rfe_os = rfe_os.fit(os_X_reply, os_y_reply.ravel())\n",
    "\n",
    "feature_reply_os = columns_reply[np.ix_(rfe_os.support_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features for reply-us:', feature_reply_us)\n",
    "print('Features for reply-os:', feature_reply_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "logreg_reply_us = LogisticRegression()\n",
    "logreg_reply_us.fit(us_X_reply_df[feature_reply_us], us_y_reply.ravel())\n",
    "\n",
    "us_y_reply_pred = logreg_reply_us.predict(X_reply_test[feature_reply_us])\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg_reply_us.score(X_reply_test[feature_us], y_reply_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_reply_us = metrics.confusion_matrix(y_reply_test, us_y_reply_pred)\n",
    "print(confusion_reply_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_reply_test, us_y_reply_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_reply_os = LogisticRegression()\n",
    "logreg_reply_os.fit(os_X_reply_df[feature_reply_os], os_y_reply.ravel())\n",
    "\n",
    "os_y_reply_pred = logreg_reply_os.predict(X_reply_test[feature_reply_os])\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg_reply_os.score(X_reply_test[feature_reply_os], y_reply_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_reply_os = metrics.confusion_matrix(y_reply_test, os_y_reply_pred)\n",
    "print(confusion_reply_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_reply_test, os_y_reply_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_reply_us_roc = metrics.roc_auc_score(y_reply_test, us_y_reply_pred)\n",
    "fpr_reply_us, tpr_reply_us, thresholds_reply_us = metrics.roc_curve(\n",
    "    y_reply_test, logreg_reply_us.predict_log_proba(X_reply_test[feature_reply_us])[:,1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_reply_us, tpr_reply_us, label='Logistic Regression (area = %0.2f)' % logreg_reply_us_roc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-\\'reply\\'-undersampling')\n",
    "plt.legend(loc=4, prop={'size': 10})\n",
    "#plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_reply_os_roc = metrics.roc_auc_score(y_reply_test, os_y_reply_pred)\n",
    "fpr_reply_os, tpr_reply_os, thresholds_reply_os = metrics.roc_curve(\n",
    "    y_reply_test, logreg_reply_os.predict_log_proba(X_reply_test[feature_reply_os])[:,1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_reply_os, tpr_reply_os, label='Logistic Regression (area = %0.2f)' % logreg_reply_os_roc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-\\'reply\\'-oversampling')\n",
    "plt.legend(loc=4, prop={'size': 10})\n",
    "#plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_reply_us = RandomForestClassifier(n_estimators=100, random_state=2019)\n",
    "forest_reply_us.fit(us_X_reply_df[feature_reply_us], us_y_reply.ravel())\n",
    "\n",
    "forest_reply_us_pred = forest_reply_us.predict(X_reply_test[feature_reply_us])\n",
    "print(metrics.classification_report(y_reply_test, forest_reply_us_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_reply_us = list(np.argsort(-forest_reply_us.feature_importances_))\n",
    "feature_reply_us[rank_reply_us]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reply_os = RandomForestClassifier(n_estimators=100, random_state=2019)\n",
    "forest_reply_os.fit(os_X_reply_df[feature_reply_os], os_y_reply.ravel())\n",
    "\n",
    "forest_reply_os_pred = forest_reply_os.predict(X_reply_test[feature_reply_os])\n",
    "print(metrics.classification_report(y_reply_test, forest_reply_os_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rank_reply_os = list(np.argsort(-forest_reply_os.feature_importances_))\n",
    "feature_reply_os[rank_reply_os]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 For `opened_at`\n",
    "### Encoding and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "drop_open = ['is_thread_reply', 'reply', 'click']\n",
    "keep_open = [i for i in data_2.columns.values.tolist() if i not in drop_open]\n",
    "\n",
    "data_open = data_2[keep_open]\n",
    "data_open.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_open = data_open.loc[:, data_open.columns != 'open']\n",
    "y_open = data_open.loc[:, data_open.columns == 'open']\n",
    "\n",
    "X_open_train, X_open_test, y_open_train, y_open_test = train_test_split(X_open, y_open, \n",
    "                                                                        test_size=0.3, random_state=2019)\n",
    "columns_open = X_open_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Under-sampling Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2 = RandomUnderSampler(random_state=2019)\n",
    "\n",
    "us_X_open, us_y_open = us2.fit_resample(X_open_train, y_open_train)\n",
    "us_X_open_df = pd.DataFrame(data=us_X_open, columns=columns_open)\n",
    "us_y_open_df = pd.DataFrame(data=us_y_open, columns=['open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Over-sampling Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os2 = SMOTE(random_state=2019)\n",
    "\n",
    "os_X_open, os_y_open = os2.fit_sample(X_open_train, y_open_train)\n",
    "os_X_open_df = pd.DataFrame(data=os_X_open, columns=columns_open)\n",
    "os_y_open_df = pd.DataFrame(data=os_y_open, columns=['open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "- RFE(Recursive Feature Elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## under-sampling\n",
    "lg_us2 = LogisticRegression()\n",
    "\n",
    "rfe_us2 = RFE(lg_us2, 20)\n",
    "rfe_us2 = rfe_us2.fit(us_X_open, us_y_open.ravel())\n",
    "\n",
    "feature_open_us = columns_open[np.ix_(rfe_us2.support_)]\n",
    "\n",
    "## over-sampling\n",
    "lg_os2 = LogisticRegression()\n",
    "\n",
    "rfe_os2 = RFE(lg_os2, 20)\n",
    "rfe_os2 = rfe_os2.fit(os_X_open, os_y_open.ravel())\n",
    "\n",
    "feature_open_os = columns_open[np.ix_(rfe_os2.support_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features for open-us:', feature_open_us)\n",
    "print('Features for open-os:', feature_open_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_open_us = LogisticRegression()\n",
    "logreg_open_us.fit(us_X_open_df[feature_open_us], us_y_open.ravel())\n",
    "\n",
    "us_y_open_pred = logreg_open_us.predict(X_reply_test[feature_open_us])\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg_open_us.score(X_open_test[feature_open_us], y_open_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_open_us = metrics.confusion_matrix(y_open_test, us_y_open_pred)\n",
    "print(confusion_open_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_open_test, us_y_open_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_open_os = LogisticRegression()\n",
    "logreg_open_os.fit(os_X_open_df[feature_open_os], os_y_open.ravel())\n",
    "\n",
    "os_y_open_pred = logreg_open_os.predict(X_reply_test[feature_open_os])\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg_open_os.score(X_open_test[feature_open_os], y_open_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_open_os = metrics.confusion_matrix(y_open_test, os_y_open_pred)\n",
    "print(confusion_open_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_open_test, os_y_open_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_open_us_roc = metrics.roc_auc_score(y_open_test, us_y_open_pred)\n",
    "fpr_open_us, tpr_open_us, thresholds_open_us = metrics.roc_curve(\n",
    "    y_open_test, logreg_open_us.predict_log_proba(X_open_test[feature_open_us])[:,1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_open_us, tpr_open_us, label='Logistic Regression (area = %0.2f)' % logreg_open_us_roc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-\\'open\\'-undersampling')\n",
    "plt.legend(loc=4, prop={'size': 10})\n",
    "#plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_open_os_roc = metrics.roc_auc_score(y_open_test, os_y_open_pred)\n",
    "fpr_open_os, tpr_open_os, thresholds_open_os = metrics.roc_curve(\n",
    "    y_open_test, logreg_open_os.predict_log_proba(X_open_test[feature_open_os])[:,1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_open_os, tpr_open_os, label='Logistic Regression (area = %0.2f)' % logreg_open_os_roc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-\\'open\\'-oversampling')\n",
    "plt.legend(loc=4, prop={'size': 10})\n",
    "#plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_open_us = RandomForestClassifier(n_estimators=100, random_state=2019)\n",
    "forest_open_us.fit(us_X_open_df[feature_open_us], us_y_open.ravel())\n",
    "\n",
    "forest_open_us_pred = forest_open_us.predict(X_open_test[feature_open_us])\n",
    "print(metrics.classification_report(y_open_test, forest_open_us_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_open_us = list(np.argsort(-forest_open_us.feature_importances_))\n",
    "feature_open_us[rank_open_us]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_open_os = RandomForestClassifier(n_estimators=100, random_state=2019)\n",
    "forest_open_os.fit(os_X_open_df[feature_open_os], os_y_open.ravel())\n",
    "\n",
    "forest_open_os_pred = forest_open_os.predict(X_open_test[feature_open_os])\n",
    "print(metrics.classification_report(y_open_test, forest_open_os_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rank_open_os = list(np.argsort(-forest_open_os.feature_importances_))\n",
    "feature_open_os[rank_open_os]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
